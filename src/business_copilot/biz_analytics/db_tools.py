import asyncio
import asyncpg
from langchain_core.tools import tool
from langchain_openai import OpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from business_copilot.biz_analytics.utils import (
    throttle,
    get_example, 
    create_table_schema)
from business_copilot.biz_analytics.pgres_utils import get_connection_pool
from business_copilot.biz_analytics.prompts import (
    DOUBLE_CHECK_PROMPT,
    ERROR_PROMPT,
    )
from business_copilot.biz_analytics.schemas import TableArgSchema, QuerySchema, ErrorQuerySchema


LLM = OpenAI(model="gpt-4o-mini", temperature=0.0)


@tool(parse_docstring=True) 
async def list_tables()-> list[str]:
    """Get all the list of tables in the database.
    
    Returns:
        List of tables in database.
    """
    pool = await get_connection_pool()
    # Get tables in database.
    async with pool.acquire() as conn:
        rows = await conn.fetch("""
            SELECT tablename
            FROM pg_catalog.pg_tables
            WHERE schemaname != 'pg_catalog' AND schemaname != 'information_schema';
        """)
        return [row['tablename'] for row in rows]


@tool(args_schema=TableArgSchema)
async def get_relevant_schema_example(table_names: list[str]) -> str:
    """Get schema and examples for requested list of tables relevant to the question."""
    
    schemas = [throttle(create_table_schema(names.lower()))
                for names in table_names]
    
    examples = [throttle(get_example(names.lower())) 
                for names in table_names]
    
    # Run table schema concurrently
    schemas = await asyncio.gather(*schemas)
    # Get examples concurrently
    examples = await asyncio.gather(*examples)
    rel_schema = ""
    for schema, eg in zip(schemas, examples):   
        rel_schema += schema + "\n\n" + eg + "\n\n"
        
    return rel_schema


@tool(args_schema=QuerySchema)
async def double_check_query(query: str) -> str:
    """Use this tool to double check if  query is correct before executing it."""

    prompt = PromptTemplate(
        input_variables=["query"], template=DOUBLE_CHECK_PROMPT
    )
    # Create chain
    chain = prompt | LLM | StrOutputParser()

    return await chain.ainvoke({"query": query})


@tool(args_schema=ErrorQuerySchema)
async def resolve_error(query: str, error: str) -> str:
    """Resolves the error that is generated by a after execution."""

    prompt = PromptTemplate(
        input_variables=["query", "error"], template=ERROR_PROMPT
    )
    # Create chain
    chain = prompt | LLM | StrOutputParser()

    return await chain.ainvoke({"query": query, "error": error})


@tool(args_schema=QuerySchema)
async def execute_query(query: str) -> str | asyncpg.Record:
    """Executes the given detailed input SQL query."""
    pool = await get_connection_pool()
    async with pool.acquire() as conn:
        try:
            values = await conn.fetch(
                query,
            )
            return values
        except Exception as e:
            return f"Error: {e}"
        finally:
            # Ensures the connection is closed after final execution.
            await conn.close()
